## GPU多卡训练总结
* [以pytorch为例，gpu多卡并行训练总结](https://bbs.cvmart.net/articles/5385)
* [Pytorch中多GPU并行计算教程](https://blog.csdn.net/qq_37541097/article/details/109736159?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164733025716780261975313%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=164733025716780261975313&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-109736159.nonecase&utm_term=GPU&spm=1018.2226.3001.4450)

## keras过拟合相关解决方法
https://mp.weixin.qq.com/s/fHDL-137qIBOvmUTfyEG8Q

1.缩小神经网络的规模
防止过拟合最简单的方法就是缩小模型的规模：模型中的可学习的参数数量。

2.权重正则化技术
一个简单的模型是指：参数值的分布具有较少的熵（或者参数较少），因此，减少过拟合的一种常见方法是通过使权重只取小值来限制网络的复杂度，这
使得权重值的分布更加规整，这就是所谓的权重正则化，这是改变网络的损失函数来实现的，在原来的损失函数基础上增加限制权重的成本，这个成本有两个：
*L1正则化*-所增加的成本与权重系数的绝对值成正比，权重稀疏化
*L2正则化*-所增加的成本与权重系数的平方成正比。L2正则化在神经网络中也成为权重衰减。

3.Dropout
Dropout是最有效和最常用的神经网络正则化技术之一。Dropout的值表示节点被随机失活的概率，通常设置为0.2到0.5之间。


## anchor如何设置？ 
https://mp.weixin.qq.com/s/2CAA4i9Nml43g5oc4p2-0Q

https://github.com/AIZOOTech/object-detection-anchors

1.什么是anchor
anchor就是在图像上预设好的不同大小，不同长宽比的参照框。

假设一个256x256大小的图片，经过64、128和256倍下采样，会产生4x4、2x2、1x1大小的特征图，我们在这三个特征图上每个点上都设置三个不同大小的anchor。
当然，这只是一个例子，实际的SSD模型，在300x300的输入下，anchor数量也特别多，其在38x38、19x19、10x10、5x5、3x3、1x1的六个特征图上，
每个点分别设置4、6、6、6、6、4个不同大小和长宽比的anchor，所以一共有38x38x4+ 19x19x6+ 10x10x6+ 5x5x6+ 3x3x4+ 1x1x4= 8732个anchor。

借助神经网络的拟合能力，我们不在需要计算Haar，HOG等特征，直接让神经网络输出，每个anchor是否包含（或者说与物体有较大重叠，也就是IOU较大）
物体，以及被检测物体相对本anchor的中心点偏移量以及长宽比例。

一般的目标检测网络可能有成千上万个anchor，例如标准的SSD在300X300输入下有8732个anchor，在500X500个anchor数量过万。我们拿上图中的三个anchor
举例，神经网络的输出，也就是每个anchor认为自己是否包含物体的概率，物体中心点与anchor自身的重点的偏移量，以及相对于anchor宽高的比例。因为anchor
的位置都是固定的，所以就可以很容易的换算出来实际物体的位置。

在训练的时候，需要anchor的大小和长宽比与待检测的物体尺寸基本一致，才可能让anchor与物体的iou为正样本， 否则，可能anchor为正样本的数目特别少，就是导致
露检很多。

如果您要检测道路两边的电线杆，电线杆的宽高比可能不止1:10，而且特别细。如果您设置的anchor长宽比为1:1、1:2、 2:1、 1:3、 3:1这五个不同的长宽比，
那可能到导致在训练的时候，没有哪个anchor与电线杆的IoU大于0.5，导致全部为负样本，那么，这样的anchor设置，模型怎么可能检测出来电线杆呢？
（虽然我们在实现SSD算法的时候，即使某个物体与所有anchor的IoU都不大于0.5的阈值，也会可怜可怜它，给它强行分配一个IoU最大的anchor，即使IoU只有0.3，但是这样，每个物体只能分配一个，而且宽高偏移量还比较大，导致回归不准。



## 深度学习优化器
* [SGD、Adam优化器](https://mp.weixin.qq.com/s/R997p1ikXS2uoBpiV-GOLQ)


## 图像增强的方法
* [keras图像增强参数详解](https://blog.csdn.net/hnu2012/article/details/54017564)
[【DeepLearning】深度学习上的图像增广（image augmentation）](https://blog.csdn.net/Taily_Duan/article/details/85417396)
* [keras实现数据的增多](https://blog.csdn.net/zhouguangfei0717/article/details/98217860)
* [增强脚本](https://blog.csdn.net/zhonglongshen/article/details/101483854)
* [深度学习图像数据增强：翻转、旋转、拉伸、五部分提取、明暗度变化python](https://blog.csdn.net/weixin_41803874/article/details/81201699)
* [图像增强综述](cnblogs.com/fydeblog/p/10734733.html)
* (https://www.cnblogs.com/abella/p/10315700.html)
* [Pytorch 中的数据增强方式最全解释](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247491791&idx=3&sn=f7ac3864154372d9d5503397d5db6b44&chksm=ec1c0d36db6b8420c6d06f086b2bf736dd41a27e34442e6bcb836bec1d3a09d66c4ae1a34b1a&mpshare=1&scene=23&srcid=&sharer_sharetime=1572491614792&sharer_shareid=41b434eda6085c4278d4ad01a4465363#rd)
* [segmentation语义分割数据增强方法及代码](https://blog.csdn.net/qq_21997625/article/details/86556667)

## python在内存中读取base64图片-skimage.io格式
https://blog.csdn.net/zhonglongshen/article/details/104523202
https://blog.csdn.net/dcrmg/article/details/80542665
[python︱matplotlib使用（读入、显示、写出、opencv混用、格式转换...）](https://blog.csdn.net/sinat_26917383/article/details/78559709)


## 为什么会产生过拟合，有哪些方法可以预防或者克服过拟合
1：增大数据量
2：较少feature个数
3：正则化
4：交叉验证，重采样评价模型效能，K折交叉验证
5：保留一个验证数据集检验

## 决策树和随机森林的区别是什么？树节点的分裂都有哪些策略？
随机森林是很多决策树组成的，有一定的避免过拟合问题
信息熵：表示信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。
1：ID3：以信息增益大小来建立
2：C4.5：以信息增益率大小来建立
3：CARD：以基尼系数大小来建立

###决策树剪枝
包含：**预剪枝和后剪枝**
预剪枝：是指在决策树的生成过程中，对每个节点在划分前后进行评估，若当前节点划分不能带来决策树的泛化性能提升，则停止划分并将当前节点标注为叶节点
后剪枝：先从训练集生成一颗完整的决策树，然后自底向上的对非叶子结点考察，若将该结点对应的子树替换成叶子结点能决策树泛化性能的提升，则将该子树替换为叶节点。


### 随机森林的优缺点
#### 优点
1.不必担心过拟合问题
2.适用于数据中存在大量未知特征
3.能够估计哪个特征在分类中更重要
4.具有很好的抗噪能力
5.可以并行处理
#### 缺点
1.对小量数据集和低纬度数据集的分类不一定可以得到很好的效果
2.可能会出现一些差异度非常小的树，淹没了一些正确的决策
3.由于树是随机生产的，结果不稳定
4.已经证明在一些噪声比较大的分类和回归问题上会过拟合
####生成步骤介绍
1.从原始训练数据中，应用bagging方法有放回的随机抽取K个新的自助样本集，并由此构建K颗分类回归树，每次未被抽到的样本组成了K个袋外数据（oob)
2.设有n个特征，则在每一颗树的每个节点处随机抽取M个特征（M<n），通过计算每个特征蕴含的信息量，特征中选择一个最具有分类能力的特征进行节点分裂
3.每棵树最大限度的生长，不做任何剪枝
4.将生成的多棵树组成随机森林，用随机森林对新的数据进行分类，分类结果按树分类器投票多少而定

#### 随机森林与梯度提升（GBDT）区别
随机森林：决策树+bagging=随机森林
梯度提升：决策树+Boosting=GBDT
两者区别在bagging和Boosting之间的区别

#### 随机森林的随机性主要体现在哪里？
1：样本抽取的随机性；2：特征抽取的随机性



## 几种优化算法

#### BGD
BGD采用整个训练集的数据来计算损失函数对参数的梯度
缺点是：由于这种方法是在一次更新中，就对整个数据集计算梯度，所以计算起来非常慢，遇到很大量的数据集也会非常棘手
而且不能投入新的数据实时更新模型。
我们会事先定义一个迭代次数epoch，首先计算梯度向量prams_grad，然后沿着梯度的方向更新参数parms，learning_rate决定了我们每一步迈多大。

#### SGD
和BGD的一次用所有数据计算梯度相比，SGD每次更新时对每个样本进行梯度更新
对于很大的数据集来说，可能会有相似的样本，这样BGD在计算梯度时会出现冗余。而SGD一次只更新一次，就没有冗余，而且比较快，并且可以新增样本
缺点是：但是SDG更新比较频繁，会造成损失函数有严重的震荡。
#### mini-batch-gradient-descent
MBGD 每一次利用一小批样本，即 n 个样本进行计算，
这样它可以降低参数更新时的方差，收敛更稳定，
另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算。
缺点是：不能保证很好的收敛性。
### Momentum
SGD方法的一个缺点是，其更新方向完全依赖于当前的batch，因而其更新十分不稳定，解决这一问题的一个简单做法就是引入Momentum
Momentum就是动量，它模拟的是物体运行时的惯性，即更新的时候是一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最新的更新方向。
这样一来，可以在一定程度上增加稳定性，从而学习的更快，并且还有一定的摆脱局部最优的能力。
### Adagrad
上面提到的方法对于所有参数都使用了同一个更新速率。但是同一个更新速率不一定适合所有参数。比如有的参数可能已经到了仅需要微调的阶段，但又有些参数由于对应样本少等原因，还需要较大幅度的调动。
Adagrad就是针对这一问题提出的，自适应地为各个参数分配不同学习率的算法。
### RMSprop
RMSprop 是 Geoff Hinton 提出的一种自适应学习率方法。
RMSprop 和 Adadelta 都是为了解决 Adagrad 学习率急剧下降问题的，

## 分类问题的几个评价指标
TP：真阳性
FP：假阳性
FN：假阴性
TN：真阴性

精确率P=真阳性/(真阳性+假阳性)
召回率R=真阳性/(真阳性+假阴性)

F1=2*(R*P)/(R+P)

### ROC曲线
ROC空间将伪阳性率（FPR）定义为X轴，真阳性率（TPR）定义为Y轴。
TPR：在所有实际为阳性的样本中，被正确的判断为阳性的比率。TPR=TP/(TP+FN)
FPT：在所有实际为阴性的样本中，被错误的判断为阳性的比率。FPR=FP/（FP+TN）
### AUC的物理意义
假设分类器的输出是样本属于正类的score（置信度），则AUC的物理意义为，任取一对样本，正样本的score大于负样本的score的概率。


### 异常值检测方法有哪些
1：箱线图法，也就是四分位数
2：z-score方法，是一种一维或者低纬度空间中的参数异常检测方法。该技术假定数据时高斯分布，异常值是分布尾部数据点，因此远离数据的平均值。
3：DBSCAN
4.孤立森林

### 样本不平衡方法
1：负样本少，就复制到一定比例
2：交叉验证
3：根据样本随机构造新的样本
4：分段逐一训练（举例：正样本10000，负样本1000，将正样本随机分成10份，每份1000，然后拿着负样本的1000与正样本的每一份进行训练，最后进行融合选择）

### 连续值转换为离散值，有什么好的方法，比如年龄？
1：根据业务知识划分
2：等频率或者等宽划分
3：风控中，可以根据woe或者iv 或者卡方检验
4：用聚类来做最优化尺度

### bagging和Boosting的区别是啥？
1：样本选择上，bagging是放回抽样，Boosting，每一轮的训练集不变，只是训练集中每个样例在分类器的权重发生变化。而权值是根据上一轮的分类结果进行调整
2：权重，bagging使用均匀取样，每个样例的权重相等。Boosting根据错误率不断调整样例的权值，错误率越大则权重越大
3：预测函数：bagging的预测函数权重相等。Boosting：每个弱分类器都有对应的权重，对于分类误差小的分类器有更大的权重
4：并行计算：





### 学习重点

centernet

DBface（focal loss，Wingloss，GIOU loss）

