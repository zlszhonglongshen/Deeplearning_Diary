* å‚è€ƒè¿æ¥ï¼Œhttps://www.cnblogs.com/wangguchangqing/archive/2004/01/13/12021638.html

* [SmoothL1](https://blog.csdn.net/ytusdc/article/details/86659696)
* [ç›®æ ‡æ£€æµ‹å›å½’æŸå¤±å‡½æ•°ç®€ä»‹ï¼šSmoothL1/IoU/GIoU/DIoU/CIoU Loss](https://zhuanlan.zhihu.com/p/104236411)


æ€»ç»“å¯¹æ¯”ä¸‹L1æŸå¤±å‡½æ•°ï¼ŒL2æŸå¤±å‡½æ•°ä»¥åŠSmoothL1æŸå¤±å‡½æ•°çš„ä¼˜ç¼ºç‚¹
# å‡æ–¹è¯¯å·®MSE (ğ¿2 Loss)
å‡æ–¹è¯¯å·®ï¼ˆMean Square Error,MSEï¼‰æ˜¯æ¨¡å‹é¢„æµ‹å€¼ğ‘“(ğ‘¥) ä¸çœŸå®æ ·æœ¬å€¼ğ‘¦ ä¹‹é—´å·®å€¼å¹³æ–¹çš„å¹³å‡å€¼ï¼Œå…¶å…¬å¼å¦‚ä¸‹

ğ‘€ğ‘†ğ¸=âˆ‘ğ‘›ğ‘–=1(ğ‘“ğ‘¥ğ‘–âˆ’ğ‘¦ğ‘–)2ğ‘›

å…¶ä¸­ï¼Œğ‘¦ğ‘–å’Œğ‘“(ğ‘¥ğ‘–)åˆ†åˆ«è¡¨ç¤ºç¬¬ğ‘–ä¸ªæ ·æœ¬çš„çœŸå®å€¼åŠå…¶å¯¹åº”çš„é¢„æµ‹å€¼ï¼Œğ‘›ä¸ºæ ·æœ¬çš„ä¸ªæ•°ã€‚

å¿½ç•¥ä¸‹æ ‡ğ‘– ï¼Œè®¾ğ‘›=1ï¼Œä»¥ğ‘“(ğ‘¥)âˆ’ğ‘¦ä¸ºæ¨ªè½´ï¼ŒMSEçš„å€¼ä¸ºçºµè½´ï¼Œå¾—åˆ°å‡½æ•°çš„å›¾å½¢å¦‚ä¸‹ï¼š



MSEçš„å‡½æ•°æ›²çº¿å…‰æ»‘ã€è¿ç»­ï¼Œå¤„å¤„å¯å¯¼ï¼Œä¾¿äºä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œæ˜¯ä¸€ç§å¸¸ç”¨çš„æŸå¤±å‡½æ•°ã€‚ è€Œä¸”ï¼Œéšç€è¯¯å·®çš„å‡å°ï¼Œæ¢¯åº¦ä¹Ÿåœ¨å‡å°ï¼Œè¿™æœ‰åˆ©äºæ”¶æ•›ï¼Œå³ä½¿ä½¿ç”¨å›ºå®šçš„å­¦ä¹ é€Ÿç‡ï¼Œä¹Ÿèƒ½è¾ƒå¿«çš„æ”¶æ•›åˆ°æœ€å°å€¼ã€‚

å½“ğ‘¦å’Œğ‘“(ğ‘¥)ä¹Ÿå°±æ˜¯çœŸå®å€¼å’Œé¢„æµ‹å€¼çš„å·®å€¼å¤§äº1æ—¶ï¼Œä¼šæ”¾å¤§è¯¯å·®ï¼›è€Œå½“å·®å€¼å°äº1æ—¶ï¼Œåˆ™ä¼šç¼©å°è¯¯å·®ï¼Œè¿™æ˜¯å¹³æ–¹è¿ç®—å†³å®šçš„ã€‚MSEå¯¹äºè¾ƒå¤§çš„è¯¯å·®ï¼ˆ>1ï¼‰ç»™äºˆè¾ƒå¤§çš„æƒ©ç½šï¼Œè¾ƒå°çš„è¯¯å·®ï¼ˆ<1ï¼‰ç»™äºˆè¾ƒå°çš„æƒ©ç½šã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹ç¦»ç¾¤ç‚¹æ¯”è¾ƒæ•æ„Ÿï¼Œå—å…¶å½±å“è¾ƒå¤§ã€‚

å¦‚æœæ ·æœ¬ä¸­å­˜åœ¨ç¦»ç¾¤ç‚¹ï¼ŒMSEä¼šç»™ç¦»ç¾¤ç‚¹æ›´é«˜çš„æƒé‡ï¼Œè¿™å°±ä¼šç‰ºç‰²å…¶ä»–æ­£å¸¸ç‚¹æ•°æ®çš„é¢„æµ‹æ•ˆæœï¼Œæœ€ç»ˆé™ä½æ•´ä½“çš„æ¨¡å‹æ€§èƒ½ã€‚ å¦‚ä¸‹å›¾ï¼š



å¯è§ï¼Œä½¿ç”¨ MSE æŸå¤±å‡½æ•°ï¼Œå—ç¦»ç¾¤ç‚¹çš„å½±å“è¾ƒå¤§ï¼Œè™½ç„¶æ ·æœ¬ä¸­åªæœ‰ 5 ä¸ªç¦»ç¾¤ç‚¹ï¼Œä½†æ˜¯æ‹Ÿåˆçš„ç›´çº¿è¿˜æ˜¯æ¯”è¾ƒåå‘äºç¦»ç¾¤ç‚¹ã€‚

å¹³å‡ç»å¯¹è¯¯å·®(ğ¿1 Loss)
å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMean Absolute Error,MAE) æ˜¯æŒ‡æ¨¡å‹é¢„æµ‹å€¼ğ‘“(ğ‘¥)å’ŒçœŸå®å€¼ğ‘¦ä¹‹é—´è·ç¦»çš„å¹³å‡å€¼ï¼Œå…¶å…¬å¼å¦‚ä¸‹ï¼š

ğ‘€ğ´ğ¸=âˆ‘ğ‘›ğ‘›=1âˆ£ğ‘“(ğ‘¥ğ‘–)âˆ’ğ‘¦ğ‘–âˆ£ğ‘›
å¿½ç•¥ä¸‹æ ‡ğ‘– ï¼Œè®¾ğ‘›=1ï¼Œä»¥ğ‘“(ğ‘¥)âˆ’ğ‘¦ä¸ºæ¨ªè½´ï¼ŒMAEçš„å€¼ä¸ºçºµè½´ï¼Œå¾—åˆ°å‡½æ•°çš„å›¾å½¢å¦‚ä¸‹ï¼š



MAEæ›²çº¿è¿ç»­ï¼Œä½†æ˜¯åœ¨ğ‘¦âˆ’ğ‘“(ğ‘¥)=0å¤„ä¸å¯å¯¼ã€‚è€Œä¸” MAE å¤§éƒ¨åˆ†æƒ…å†µä¸‹æ¢¯åº¦éƒ½æ˜¯ç›¸ç­‰çš„ï¼Œè¿™æ„å‘³ç€å³ä½¿å¯¹äºå°çš„æŸå¤±å€¼ï¼Œå…¶æ¢¯åº¦ä¹Ÿæ˜¯å¤§çš„ã€‚è¿™ä¸åˆ©äºå‡½æ•°çš„æ”¶æ•›å’Œæ¨¡å‹çš„å­¦ä¹ ã€‚ä½†æ˜¯ï¼Œæ— è®ºå¯¹äºä»€ä¹ˆæ ·çš„è¾“å…¥å€¼ï¼Œéƒ½æœ‰ç€ç¨³å®šçš„æ¢¯åº¦ï¼Œä¸ä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œå…·æœ‰è¾ƒä¸ºç¨³å¥æ€§çš„è§£ã€‚

ç›¸æ¯”äºMSEï¼ŒMAEæœ‰ä¸ªä¼˜ç‚¹å°±æ˜¯ï¼Œå¯¹äºç¦»ç¾¤ç‚¹ä¸é‚£ä¹ˆæ•æ„Ÿã€‚å› ä¸ºMAEè®¡ç®—çš„æ˜¯è¯¯å·®ğ‘¦âˆ’ğ‘“(ğ‘¥)çš„ç»å¯¹å€¼ï¼Œå¯¹äºä»»æ„å¤§å°çš„å·®å€¼ï¼Œå…¶æƒ©ç½šéƒ½æ˜¯å›ºå®šçš„ã€‚

é’ˆå¯¹ä¸Šé¢å¸¦æœ‰ç¦»ç¾¤ç‚¹çš„æ•°æ®ï¼ŒMAEçš„æ•ˆæœè¦å¥½äºMSEã€‚



æ˜¾ç„¶ï¼Œä½¿ç”¨ MAE æŸå¤±å‡½æ•°ï¼Œå—ç¦»ç¾¤ç‚¹çš„å½±å“è¾ƒå°ï¼Œæ‹Ÿåˆç›´çº¿èƒ½å¤Ÿè¾ƒå¥½åœ°è¡¨å¾æ­£å¸¸æ•°æ®çš„åˆ†å¸ƒæƒ…å†µã€‚

MSEå’ŒMAEçš„é€‰æ‹©
ä»æ¢¯åº¦çš„æ±‚è§£ä»¥åŠæ”¶æ•›ä¸Šï¼ŒMSEæ˜¯ç”±äºMAEçš„ã€‚MSEå¤„å¤„å¯å¯¼ï¼Œè€Œä¸”æ¢¯åº¦å€¼ä¹Ÿæ˜¯åŠ¨æ€å˜åŒ–çš„ï¼Œèƒ½å¤Ÿå¿«é€Ÿçš„æ”¶æ•›ï¼›è€ŒMAEåœ¨0ç‚¹å¤„ä¸å¯å¯¼ï¼Œä¸”å…¶æ¢¯åº¦ä¿æŒä¸å˜ã€‚å¯¹äºå¾ˆå°çš„æŸå¤±å€¼å…¶æ¢¯åº¦ä¹Ÿå¾ˆå¤§ï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå°±éœ€è¦ä½¿ç”¨å˜åŒ–çš„å­¦ä¹ ç‡ï¼Œåœ¨æŸå¤±å€¼å¾ˆå°æ—¶é™ä½å­¦ä¹ ç‡ã€‚

å¯¹ç¦»ç¾¤ï¼ˆå¼‚å¸¸ï¼‰å€¼å¾—å¤„ç†ä¸Šï¼ŒMAEè¦æ˜æ˜¾å¥½äºMSEã€‚

å¦‚æœç¦»ç¾¤ç‚¹ï¼ˆå¼‚å¸¸å€¼ï¼‰éœ€è¦è¢«æ£€æµ‹å‡ºæ¥ï¼Œåˆ™å¯ä»¥é€‰æ‹©MSEä½œä¸ºæŸå¤±å‡½æ•°ï¼›å¦‚æœç¦»ç¾¤ç‚¹åªæ˜¯å½“åšå—æŸçš„æ•°æ®å¤„ç†ï¼Œåˆ™å¯ä»¥é€‰æ‹©MAEä½œä¸ºæŸå¤±å‡½æ•°ã€‚

æ€»ä¹‹ï¼ŒMAEä½œä¸ºæŸå¤±å‡½æ•°æ›´ç¨³å®šï¼Œå¹¶ä¸”å¯¹ç¦»ç¾¤å€¼ä¸æ•æ„Ÿï¼Œä½†æ˜¯å…¶å¯¼æ•°ä¸è¿ç»­ï¼Œæ±‚è§£æ•ˆç‡ä½ã€‚å¦å¤–ï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ”¶æ•›è¾ƒæ…¢ã€‚MSEå¯¼æ•°æ±‚è§£é€Ÿåº¦é«˜ï¼Œä½†æ˜¯å…¶å¯¹ç¦»ç¾¤å€¼æ•æ„Ÿï¼Œä¸è¿‡å¯ä»¥å°†ç¦»ç¾¤å€¼çš„å¯¼æ•°è®¾ä¸º0ï¼ˆå¯¼æ•°å€¼å¤§äºæŸä¸ªé˜ˆå€¼ï¼‰æ¥é¿å…è¿™ç§æƒ…å†µã€‚

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸Šè¿°ä¸¤ç§æŸå¤±å‡½æ•°éƒ½ä¸èƒ½æ»¡è¶³éœ€æ±‚ã€‚ä¾‹å¦‚ï¼Œè‹¥æ•°æ®ä¸­90%çš„æ ·æœ¬å¯¹åº”çš„ç›®æ ‡å€¼ä¸º150ï¼Œå‰©ä¸‹10%åœ¨0åˆ°30ä¹‹é—´ã€‚é‚£ä¹ˆä½¿ç”¨MAEä½œä¸ºæŸå¤±å‡½æ•°çš„æ¨¡å‹å¯èƒ½ä¼šå¿½è§†10%çš„å¼‚å¸¸ç‚¹ï¼Œè€Œå¯¹æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹å€¼éƒ½ä¸º150ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹ä¼šæŒ‰ä¸­ä½æ•°æ¥é¢„æµ‹ã€‚è€Œä½¿ç”¨MSEçš„æ¨¡å‹åˆ™ä¼šç»™å‡ºå¾ˆå¤šä»‹äº0åˆ°30çš„é¢„æµ‹å€¼ï¼Œå› ä¸ºæ¨¡å‹ä¼šå‘å¼‚å¸¸ç‚¹åç§»ã€‚

è¿™ç§æƒ…å†µä¸‹ï¼ŒMSEå’ŒMAEéƒ½æ˜¯ä¸å¯å–çš„ï¼Œç®€å•çš„åŠæ³•æ˜¯å¯¹ç›®æ ‡å˜é‡è¿›è¡Œå˜æ¢ï¼Œæˆ–è€…ä½¿ç”¨åˆ«çš„æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚ï¼šHuber,Log-Coshä»¥åŠåˆ†ä½æ•°æŸå¤±ç­‰ã€‚

# Smooth ğ¿1 Loss
åœ¨Faster R-CNNä»¥åŠSSDä¸­å¯¹è¾¹æ¡†çš„å›å½’ä½¿ç”¨çš„æŸå¤±å‡½æ•°éƒ½æ˜¯Smooth ğ¿1 ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œ

Smoothğ¿1(ğ‘¥)={0.5ğ‘¥2âˆ£ğ‘¥âˆ£âˆ’0.5ğ‘–ğ‘“âˆ£ğ‘¥âˆ£<1ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’
å…¶ä¸­ï¼Œğ‘¥=ğ‘“(ğ‘¥ğ‘–)âˆ’ğ‘¦ğ‘– ä¸ºçœŸå®å€¼å’Œé¢„æµ‹å€¼çš„å·®å€¼ã€‚

Smooth ğ¿1 èƒ½ä»ä¸¤ä¸ªæ–¹é¢é™åˆ¶æ¢¯åº¦ï¼š

å½“é¢„æµ‹æ¡†ä¸ ground truth å·®åˆ«è¿‡å¤§æ—¶ï¼Œæ¢¯åº¦å€¼ä¸è‡³äºè¿‡å¤§ï¼›
å½“é¢„æµ‹æ¡†ä¸ ground truth å·®åˆ«å¾ˆå°æ—¶ï¼Œæ¢¯åº¦å€¼è¶³å¤Ÿå°ã€‚
å¯¹æ¯”ğ¿1 Loss å’Œ ğ¿2 Loss
å…¶ä¸­ğ‘¥ä¸ºé¢„æµ‹æ¡†ä¸groud truthä¹‹é—´çš„å·®å¼‚ï¼š

ğ¿2(ğ‘¥)ğ¿1(ğ‘¥)ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„ğ¿1(ğ‘¥)=ğ‘¥2=ğ‘¥={0.5ğ‘¥2âˆ£ğ‘¥âˆ£âˆ’0.5ğ‘–ğ‘“âˆ£ğ‘¥âˆ£<1ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’(1)(2)(3)
ä¸Šé¢æŸå¤±å‡½æ•°å¯¹ğ‘¥çš„å¯¼æ•°ä¸ºï¼š

âˆ‚ğ¿2(ğ‘¥)âˆ‚ğ‘¥âˆ‚ğ¿1(ğ‘¥)âˆ‚ğ‘¥âˆ‚ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„ğ¿1(ğ‘¥)âˆ‚ğ‘¥=2ğ‘¥={1âˆ’1if ğ‘¥â‰¥0otherwise={ğ‘¥Â±1ğ‘–ğ‘“âˆ£ğ‘¥âˆ£<1ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’(4)(5)(6)
ä¸Šé¢å¯¼æ•°å¯ä»¥çœ‹å‡ºï¼š

æ ¹æ®å…¬å¼-4ï¼Œå½“ğ‘¥å¢å¤§æ—¶ï¼Œğ¿2çš„æŸå¤±ä¹Ÿå¢å¤§ã€‚ è¿™å°±å¯¼è‡´åœ¨è®­ç»ƒåˆæœŸï¼Œé¢„æµ‹å€¼ä¸ groud truth å·®å¼‚è¿‡äºå¤§æ—¶ï¼ŒæŸå¤±å‡½æ•°å¯¹é¢„æµ‹å€¼çš„æ¢¯åº¦ååˆ†å¤§ï¼Œè®­ç»ƒä¸ç¨³å®šã€‚

æ ¹æ®å…¬å¼-5,ğ¿1å¯¹ğ‘¥çš„å¯¼æ•°ä¸ºå¸¸æ•°ï¼Œåœ¨è®­ç»ƒçš„åæœŸï¼Œé¢„æµ‹å€¼ä¸ground truthå·®å¼‚å¾ˆå°æ—¶ï¼Œğ¿1çš„å¯¼æ•°çš„ç»å¯¹å€¼ä»ç„¶ä¸º1ï¼Œè€Œ learning rate å¦‚æœä¸å˜ï¼ŒæŸå¤±å‡½æ•°å°†åœ¨ç¨³å®šå€¼é™„è¿‘æ³¢åŠ¨ï¼Œéš¾ä»¥ç»§ç»­æ”¶æ•›ä»¥è¾¾åˆ°æ›´é«˜ç²¾åº¦ã€‚

æ ¹æ®å…¬å¼-6ï¼ŒSmotth ğ¿1åœ¨ğ‘¥è¾ƒå°æ—¶ï¼Œå¯¹ğ‘¥çš„æ¢¯åº¦ä¹Ÿä¼šå˜å°ã€‚ è€Œå½“ğ‘¥è¾ƒå¤§æ—¶ï¼Œå¯¹ğ‘¥çš„æ¢¯åº¦çš„ä¸Šé™ä¸º1ï¼Œä¹Ÿä¸ä¼šå¤ªå¤§ä»¥è‡³äºç ´åç½‘ç»œå‚æ•°ã€‚ğ‘†ğ‘šğ‘œğ‘œğ‘¡â„ğ¿1å®Œç¾çš„é¿å¼€äº†ğ¿1å’Œğ¿2ä½œä¸ºæŸå¤±å‡½æ•°çš„ç¼ºé™·ã€‚

ğ¿1 Loss ,ğ¿2 Lossä»¥åŠğ‘†ğ‘šğ‘œğ‘œğ‘¡â„ğ¿1 æ”¾åœ¨ä¸€èµ·çš„å‡½æ•°æ›²çº¿å¯¹æ¯”



ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼Œè¯¥å‡½æ•°å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªåˆ†æ®µå‡½æ•°ï¼Œåœ¨[-1,1]ä¹‹é—´å®é™…ä¸Šå°±æ˜¯L2æŸå¤±ï¼Œè¿™æ ·è§£å†³äº†L1çš„ä¸å…‰æ»‘é—®é¢˜ï¼Œåœ¨[-1,1]åŒºé—´å¤–ï¼Œå®é™…ä¸Šå°±æ˜¯L1æŸå¤±ï¼Œè¿™æ ·å°±è§£å†³äº†ç¦»ç¾¤ç‚¹æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜

å®ç° (PyTorch)
def _smooth_l1_loss(input, target, reduction='none'):
    # type: (Tensor, Tensor) -> Tensor
    t = torch.abs(input - target)
    ret = torch.where(t < 1, 0.5 * t ** 2, t - 0.5)
    if reduction != 'none':
        ret = torch.mean(ret) if reduction == 'mean' else torch.sum(ret)
    return ret      
ä¹Ÿå¯ä»¥æ·»åŠ ä¸ªå‚æ•°beta è¿™æ ·å°±å¯ä»¥æ§åˆ¶ï¼Œä»€ä¹ˆèŒƒå›´çš„è¯¯å·®ä½¿ç”¨MSEï¼Œä»€ä¹ˆèŒƒå›´å†…çš„è¯¯å·®ä½¿ç”¨MAEäº†ã€‚

def smooth_l1_loss(input, target, beta=1. / 9, reduction = 'none'):
    """
    very similar to the smooth_l1_loss from pytorch, but with
    the extra beta parameter
    """
    n = torch.abs(input - target)
    cond = n < beta
    ret = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)
    if reduction != 'none':
        ret = torch.mean(ret) if reduction == 'mean' else torch.sum(ret)
    return ret
æ€»ç»“
å¯¹äºå¤§å¤šæ•°CNNç½‘ç»œï¼Œæˆ‘ä»¬ä¸€èˆ¬æ˜¯ä½¿ç”¨L2-lossè€Œä¸æ˜¯L1-lossï¼Œå› ä¸ºL2-lossçš„æ”¶æ•›é€Ÿåº¦è¦æ¯”L1-lossè¦å¿«å¾—å¤šã€‚

å¯¹äºè¾¹æ¡†é¢„æµ‹å›å½’é—®é¢˜ï¼Œé€šå¸¸ä¹Ÿå¯ä»¥é€‰æ‹©å¹³æ–¹æŸå¤±å‡½æ•°ï¼ˆL2æŸå¤±ï¼‰ï¼Œä½†L2èŒƒæ•°çš„ç¼ºç‚¹æ˜¯å½“å­˜åœ¨ç¦»ç¾¤ç‚¹ï¼ˆoutliers)çš„æ—¶å€™ï¼Œè¿™äº›ç‚¹ä¼šå lossçš„ä¸»è¦ç»„æˆéƒ¨åˆ†ã€‚æ¯”å¦‚è¯´çœŸå®å€¼ä¸º1ï¼Œé¢„æµ‹10æ¬¡ï¼Œæœ‰ä¸€æ¬¡é¢„æµ‹å€¼ä¸º1000ï¼Œå…¶ä½™æ¬¡çš„é¢„æµ‹å€¼ä¸º1å·¦å³ï¼Œæ˜¾ç„¶losså€¼ä¸»è¦ç”±1000å†³å®šã€‚æ‰€ä»¥FastRCNNé‡‡ç”¨ç¨å¾®ç¼“å’Œä¸€ç‚¹ç»å¯¹æŸå¤±å‡½æ•°ï¼ˆsmooth L1æŸå¤±ï¼‰ï¼Œå®ƒæ˜¯éšç€è¯¯å·®çº¿æ€§å¢é•¿ï¼Œè€Œä¸æ˜¯å¹³æ–¹å¢é•¿ã€‚

ã€€ã€€Smooth L1 å’Œ L1 Loss å‡½æ•°çš„åŒºåˆ«åœ¨äºï¼ŒL1 Loss åœ¨0ç‚¹å¤„å¯¼æ•°ä¸å”¯ä¸€ï¼Œå¯èƒ½å½±å“æ”¶æ•›ã€‚Smooth L1çš„è§£å†³åŠæ³•æ˜¯åœ¨ 0 ç‚¹é™„è¿‘ä½¿ç”¨å¹³æ–¹å‡½æ•°ä½¿å¾—å®ƒæ›´åŠ å¹³æ»‘ã€‚

**Smooth L1çš„ä¼˜ç‚¹**

ç›¸æ¯”äºL1æŸå¤±å‡½æ•°ï¼Œå¯ä»¥æ”¶æ•›å¾—æ›´å¿«ã€‚
ç›¸æ¯”äºL2æŸå¤±å‡½æ•°ï¼Œå¯¹ç¦»ç¾¤ç‚¹ã€å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼Œæ¢¯åº¦å˜åŒ–ç›¸å¯¹æ›´å°ï¼Œè®­ç»ƒæ—¶ä¸å®¹æ˜“è·‘é£ã€‚