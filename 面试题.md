* https://www.nowcoder.com/discuss/128148
* https://blog.csdn.net/liuxiao214/article/details/83043170
* https://blog.csdn.net/qq_39056987/article/details/112104199
* https://blog.csdn.net/qq_39056987/article/details/112156926
* CNN调优总结：https://mp.weixin.qq.com/s/UJZm3A54qeWLzAMXQ_x05Q

- 常见的初始化方法
- 写一下leaky ReLU的公式
- 介绍YOLO系列的损失函数
- Anchor-free的优势在哪里？
- Adam用到二阶矩的原理是什么？

- 介绍跨卡同步BN
- Canny算子的原理
- GBDT的G梯度的向量长度为多少？
- C++底层代码实现图像3*3均值滤波
- One-stage和two-stage检测网络区别

- Triplet loss介绍
- HOG特征的计算过程
- YOLOv4的数据增强方法
- 介绍SIFT算法，具有哪些不变性？
- AdaBoost和随机森林有什么区别？

- mIoU计算方法
- 介绍BN及其变体
- ROI pooling 作用
- 朴素贝叶斯，为什么朴素？
- 画一下Transformer结构图


- 介绍ReLU和变体
- 如何可视化特征图？
- ROC曲线的横纵轴是什么？
- 卷积层参数量运算量的计算
- 为什么Transformer需要多头注意力?

- 介绍RoI Pooling
- LightGBM的原理
- Transformer的结构介绍
- 正则化L1和L2有什么区别？
- XGBoost是怎么选择最优分裂点的？


- 介绍残差块的结构
- Adam和SGD的区别
- 为什么GAN很难训练？
- 越大的BatchSize就越好吗？
- K-means与DBSCAN的区别

- 介绍EfficientNet
- NMS的作用和步骤
- 写一下Focal Loss公式
- GBDT和XGBoost的区别
- 如何理解Self-Attention中的Q, K, V？



1.传统图像算法问题
图像预处理有哪些方法？
图像增强有哪些方法？
直方图均衡介绍。
膨胀和腐蚀含义?开运算和闭运算先后顺序。
传统的边缘检测算子有哪些？
Canny算法介绍。如何极大值抑制？（重点必问）
hog和haar介绍。
高斯滤波器原理。
如何对图像进行90度旋转？（笔试题）
给你一个图如何提取想要的目标。
（这是一个开放性的题目，需要对传统图像算法有一定的理解 ）

列举常用的机器学习算法。（说的越多问的越多）
决策树原理介绍。预剪枝和后剪枝处理简介。
决策树与Adaboost区别。
SVM介绍，如何理解最大支持向量，核函数作用。
贝叶斯分类器简绍。
sigmoid和relu区别和优点，以及运用场合。
逻辑回归介绍。（有一个小公司变态到让我手写逻辑回归推理）
k-means和knn介绍，并说明两者区别。
无监督学习和有监督学习区别并列举各有哪些算法。（笔试题）
bp神经网络介绍。
————————————————
ResNet网络介绍，与其余网络区别，手写ResNet残差块。（重点必问）
激活函数有哪些及作用。为什么使用relu不使用sigmoid？
梯度消失和梯度爆炸阐述，并讲解如何解决。
如何知道网络训练有没有过拟合，并如何解决。（重点必问）
分割网络有哪些？各有什么特点。
检测网络有哪些，各有什么特点。
阐述Faster-Rcnn，YOLO和SSD网络，并介绍各有什么优缺点。
OpenCV和Halcon是加分项。（我对这块接触的不多，每次问道这个问题我都懵圈了，但是一定要表达一下自己自学过，只是没有在项目中用到过。面试官就不会为难你了）
计算网络参数量以及特征图输出计算公式。
RNN和LSTM 介绍。
除了CNN网络还了解哪些网络？
BN层如何对数据处理。作用是什么？



### https://blog.csdn.net/qq_39056987/article/details/112157031

为什么需要做归一化、标准化？

常用的归一化和标准化的方法有哪些？

简单介绍一下空洞卷积的原理和作用？

为什么线性回归使用mse作为损失函数？

怎么判断模型是否过拟合？有哪些防止过拟合的策略？


除了SGD和Adam之外，你还有知道哪些优化算法？

说说模型训练的调参技巧？

阐释一下感受野的概念？

神经网络的深度和宽度分别指的是什么？


模型的参数量指的是什么？怎么计算？

mobilnetV1 mobilenetV2 mobilenetV3 

下采样的作用是什么？通常有哪些方式？

上采样的原理和常用方式？

模型的FLOPS（计算量）指的是什么？怎么计算？

深度可分离卷积的概念和作用？

转置卷积的原理？

神经网络的中的Addition/concatenate区别是什么？

你知道哪些常用的激活函数？

神经网络中1*1卷积有什么作用？

* 解释一下目标检测中的anchor机制？他的作用是什么？

  

BN的原理和作用是什么？

随机梯度下降相比全局梯度下降好处是什么？

如果在网络初始化时给网络赋予0的权重，这个网络能正常训练嘛？

无监督学习有哪些？

答：强化学习，kmean聚类、自编码、受限玻尔兹曼机

增大感受野的方法？

答：空洞卷积、池化操作、角度卷积核尺寸的卷积操作。

神经网络的正则化方法？过拟合的解决方法？

答：数据增强；early stopping（比较训练损失和验证损失曲线，验证损失最小即为最优迭代次数）；L2正则化；L1正则化；dropout正则化

* 梯度消失和梯度爆炸的原因是什么？

  原因：激活函数的选择

  梯度消失：令bias=0，则神经网络的输出结果等于各层权重参数的积再与输入数据集相乘，若参数值较小时，则权重参数呈指数级减小。

  梯度爆炸：令bias=0，则神经网络的输出结果等于各层权重参数的积再与输入数据集相乘，若参数值较大时，则权重参数呈指数级增长。

* 深度学习为什么在计算机领域这么好？

  以目标检测为例，传统的计算机视觉方法需首先基于经验手动设计特横，然后使用分类器分类，这两个过程都是分开的。而深度学习力的卷积网络可实现对局部领域信息的提取，获得更高级的特征，当神经网络层数越多时，提取的特征更抽象，将更有利于分类，同时神经网络提取特征和分类融合在一个结构中。

* 为什么神经网络常用relu作为激活函数？

  1.在前向传播和反向传播过程中，relu相比于sigmoid等激活函数计算量小

  2.在反向传播过程中，sigmoid函数存在饱和区，若激活值进入饱和区，则其梯度更新值非常小，导致出现梯度消失的现象，而relu没有饱和区，可避免次问题

  3.relu可令部分神经元输出为0，造成网络的稀疏性，减少前后层参数对当前层参数的影响，提升了模型的泛化性能。

* 卷积层和全连接侧层的区别是什么?

  1.卷积层是局部链接，所以提取的是局部信息，全连接层是全局链接，所以提取的是全局信息

  2.当卷积层的局部链接是全局连接时，全连接层是卷积层的特例。

* 什么是正则化，L1正则化和L2正则化有什么区别？

* 常用的模型压缩方式有哪些

  1.使用轻量型的特征提取网络，例如mobilenet、shufflenet、ghostnet系列等等

  2.通道，层剪枝

  3.模型蒸馏

  4.模型量化

* 介绍一下mobilnet轻量型网络的设计网络

* 介绍一下shufflenet轻量网络的设计思想，他和mobilnet有什么区别？ 

* yoloV3相对于yoloV2做了哪些改进？v4相对于V3有哪些改进？

* 介绍一下残差网络的设计思想，残差主要是用来解决什么问题？他是怎么去做的？

* 最近看了什么论文，能否简单介绍一下论文内容

* 如何处理样本不平衡问题

* 解释bias和Variance之间的权衡关系

* 如果深度学习模型中已经有了1000万张人脸向量，如果通过查询来找到新的人脸？

* 对于分类问题，精度指标是否完全可靠？你通常使用哪些度量来评估你的模型？

* 如何理解反向传播？解释其作用机制

  问题旨在检测对于神经网络如何工作的知识：

  * 前向过程：是一个帮助模型计算每一层权重的过程，其计算结果将产生一个
  * 反向求导：为了降低损失函数的值，我们需要使用导数。
  * 反向传播：使用链式规则或导数函数计算每一层从最后一层到第一层的梯度值
  * 反复迭代：通过反复迭代上述过程，降低模型损失，从而达到收敛效果。

* 激活函数是什么意思？激活函数的饱和区间是多少？

  神经网络是由多个神经元构成，每个神经元都是一个线性单元，可以用来解决简单线性可分问题，但是在实际情况中，我们处理的问题往往不是线性的，所以需要引进非线性的激活函数，给模型提供了非线性的能力，从而能更好的解决实际问题，常用的激活函数有：

  * sigmoid

  * tanh

  * RElu

  * PRlu

  * LeakRelu

    其中sigmoid，relu和tanh都存在饱和区间，是饱和性激活函数，饱和区间可以理解为：触发函数的饱和范围是指即使输入值改变，函数的输出值也不改变的区间

    ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200329153742653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5MDU2OTg3,size_16,color_FFFFFF,t_70)

* 学习率太高或者太低会怎么样？

  * 学习率太低：

    模型训练将会进行得非常慢，因为它对权重进行非常小的更新。在到达局部最优点之前需要多次更新。
    学习率太高：

    由于权值更新过大，模型可能会不收敛。有可能在一个更新权值的步骤中，模型跳出了局部优化，使得模型以后很难更新到最优点，而是在在局部优化点附近跳来跳去（震荡）。
    实际使用：

    通常在模型训练开始阶段会给一个比较大的学习率（比如0.1，0.01等），在模型训练越靠后，使用学习率衰减策略（每多少轮次学习率×0.1等），减小学习率，从而使得能够更好的收敛。

* 当图像尺寸变为2倍，CNN的参数数量变为几倍？为什么？

  答案是**不变**！！

  模型参数量和图像大小没关系，只和网络结构中卷积核数量和大小有关系，增加的是**计算量**！！

* 如何处理不平衡数据？

* 模型训练中，Epoch，Batch和iteration概念

  Epoch：表示整个数据集的迭代（所有内容都包含在训练模型中）

  Batch：是指当我们不能一次性将整个数据集放到神经网络中，我们将数据集分割成几批较小数据集

  Iteration：是运行epoch所需的批数。假设有 10,000 个图像作为数据，批处理的大小(batch_size)为 200。然后一个 epoch 将包含 50 个Iteration(10,000 除以 200)。

* 介绍BN的意义

  BN是训练神经网络模型的一种有效方法。该方法的目标是将特征（每层激活后的输出）归一化为均值为0，标准差为1,。它可以将数据标准化到一个合理的范围，这样做带来的好处有：

  * 可以使用更大的学习率，从而加速模型收敛：如果x(输入数据)带来不稳定的变化，其导数可能太大，也可能太小，导致学习模型不稳定，BN可以将不稳定的数据归一化到合理的范围；
  * 可以避免x值经过非线性激活函数后趋于饱和的现象：有助于权重的学习，当不使用时有些权重可能永远无法进行学习，而用了之后，基本上都可以学习到；
  * 有效的减少过拟合现象：BN也是一种正则化形式，有助于最小化过拟合，使用BN，我们不需要使用太多的dropout

  

- GAN和VAE的区别
  <u>\- 介绍NMS原理和伪代码</u>
  <u>\- YOLOv1-YOLOv3的改进点</u>
  \- 介绍Faster R-CNN的损失函数
  \- DCGAN和原始GAN有什么区别？
  \- 介绍ResNeXt网络
  \- xgb的boosting如何体现？
  \- BN一般放在哪里？为什么？
  \- 介绍DeepLab系列网络和Auto版本
  \- 介绍传统和基于NAS的数据增广方法
  \- 介绍BERT及其变种
  \- 手推 LR 的损失函数
  \- 决策树如何剪枝和分裂的？
  \- 常见的图像插值方法有哪些？
  \- 介绍有监督、自监督和半监督学习
- 仿射变换有几个自由度？

<u>\- 如何提高小目标检测性能？</u>

\- 注意力机制最初提出是为了做什么的？

\- BN/IN/WN/GN/LN的区别及适用场景

\- Python中的生成器和迭代器的区别？

\- 介绍U-Net及其变体

\<u>- 写一下smooth L1的公式</u>

<u>\- 1*1的卷积核有哪些作用？</u>

\- 介绍CAM模型可视化及其变体

\<u>- Adam怎么实现自适应学习率？</u>

<u>\- 数据增广方法有哪些？</u>

\- 介绍Caffe中卷积的实现

\- pooling 层怎么反向传播？

\- 图像处理中平滑和锐化操作是什么？

<u>\- Attention的起源，以及是用在哪里？</u>

\- 介绍直方图均衡化

\- 手撕IoU和NMS代码

\- 介绍图像的高频、低频分量/信号

\- 生成模型和判别模型分别有哪些？

\- 如何解决多标签分类问题？介绍损失函数

\- 介绍高斯滤波的步骤

\- 单应性（homography）原理

\- BN 可以防止过拟合么？为什么

\- 如何做目标检测的多尺度训练/测试？

\- Squeeze-Excitation结构怎么实现的？

\- 加速网络收敛的办法

\- 哪几种滤波器是平滑的？

\- 介绍label assigning方面的工作

\- 介绍跨卡同步Batch Normalization

\- YOLOv4使用的数据增广有哪些方法？

\- 手撕logistic回归

\- 如何实现Dropout？

\- Self-attention原理

\- 如何判断聚类效果的好坏？

\- 介绍并评价目标检测新范式DETR？

\- 写BatchNorm的公式

\- 聚类评价指标有哪些？

\- 较新的激活函数有哪些？

\- YOLOv3中 route层的作用是什么？

\- 介绍IoU、GIoU、DIoU、CIoU损失函数

\- 什么是图像直方图？

\- 权重初始化方法有哪些？

\- BN层反向传播，怎么求导？

\- 介绍mAP、PR曲线、AUC的定义

\- 普通卷积、DW PW卷积计算量推导

\- 介绍常见的聚类方法

\- 自监督和无监督的区别

\- 检测任务是分类任务还是回归任务？

\- RPN 中正类和负类的筛选阈值是什么？

\- 要同时使用BN和dropout，该如何使用？

\- 如何实现多任务网络？

\- One-hot有什么作用？

\- 介绍Caffe框架的工厂模式

\- SVM推导以及如何解决多分类问题

\- anchor的意义，anchor-free的意义

\- Sobel算子介绍一下

\- BN层的定义以及功能（写公式）

\- 解决数据不平衡的损失函数有哪些？

\- Focal Loss解决什么问题？有多少超参数？如何调参？

\- 写一下Self-attention公式

\- 朴素贝叶斯的朴素是什么意思？

\- 介绍AUC值，PR曲线，ROC曲线

\- 卷积神经网络的卷积核为什么大多是方形？而且是奇数？

\- 写一下mAP公式

\- 画一下VGG-16网络

\- 手撕K-means伪代码

\- 最大池化和均值池化适用的场景？

\- ShuffleNetv2相比v1有什么改进？

\- 怎么解决长尾问题？

\- 神经网络如何跳出局部最优？

\- 画一下ShuffleNetv2的结构

\- Channel Attention怎么实现的？

\- 多分类如果有 10000 类别，怎么优化？



\- 手撕XGBoost算法

\- 介绍开运算、闭运算

\- 介绍双线性插值（画图）

\- Early stopping怎么做的？

\- SVM核函数的作用，如何选取核函数？

\- 介绍DBSCAN聚类算法

\- 介绍分割网络的损失函数

\- 介绍目标检测中的多尺度训练/测试

\- bbox目标大小尺寸差异巨大怎么解决？损失函数上怎么设计？

\- 介绍霍夫变换（画出来）

\- pooling 层怎么反向传播？

\- Attention起源是用在哪里？

\- 介绍Mask R-CNN的RoI Align

\- ResNet第二个版本做了哪些改进？

\- 不同卷积核大小意义

\- 神经网络如何可视化？

\- CART树怎么分裂节点？

\- YOLOv3中bbox坐标回归怎么做的？和Faster R-CNN有什么区别？

\- 不平衡样本怎么处理？

\- 介绍常见的激活函数（画出来）

\- 目标检测中正负样本如何选取？

\- K-means的初始中心怎么优化？

\- CenterNet的heatmap怎么生成？

\- 绍介‬‬SIFT的理原‬‬和特性

\- AUC法方算计‬‬与Python现实‬‬

\- 画一下ResNet和VGG网络（画简‬‬即可）

\- 3*3 换变何几‬‬矩阵中的参数是什么含义？

\- 你做标目‬‬检测一般用什么损失函数？么什为‬‬用这个？



\- 绍介‬‬SIFT的理原‬‬和特性

\- AUC法方算计‬‬与Python现实‬‬

\- 画一下ResNet和VGG网络（画简‬‬即可）

\- 3*3 换变何几‬‬矩阵中的参数是什么含义？

\- 你做标目‬‬检测一般用什么损失函数？么什为‬‬用这个？

\- 1*1卷积的作用

\- 介绍边缘检测算子

\- 用NumPy实现卷积

\- 手写高斯滤波以及优化

\- 思考题：褶皱的纸，如何变形至平坦？


\- 核函数有哪些？

\- 介绍PCA与SVD

\- 如何计算网络的参数量

\- 介绍感受野含义和计算公式

\- 卷积神经网络提取的特征是什么?

\- 介绍三种图像插值方法

\- CNN网络中的不变性理解

\- One stage与Two Stage的区别

\- ResNet 残差网络解决的是什么问题？为什么能解决？

\- 介绍HOG算法

\- 知道哪些损失函数？

\- 介绍你知道的优化器

\- 训练时出现loss NAN的可能因素

\- 介绍Attention的QKV，怎么计算的？

\- GDBT的基本原理

\- 推导神经网络链式法则

\- 手撕代码：二叉树最大宽度

\- 模型加速优化的方法有哪些？

\- 介绍Adam优化算法，有哪些参数？

\- 手推SVM

\- RF和GBDT的区别

\- 常见的色彩空间有哪些？

\- point wise 和pair wise的优缺点

\- 反转链表，要求空间复杂度O(1)

\- 介绍膨胀卷积

\- 手推卷积的过程

\- Mini-batch的作用

\- 介绍相机内参外参标定原理

\- 逻辑回归为什么不用MSE做损失函数



\- 学习率调节策略

\- SVM引入核函数本质？

\- 决策树怎么选择特征？

\- C++中什么是左值什么是右值

\- 图像质量评价指标PSNR、SSIM

\- 介绍BN/LN/IN/GN

\- 介绍反卷积及其应用

\- 如何解决数据少的问题？

\- 从两种数学角度解释L1和L2

\- 算法题：两字符串最长公共子序列



\- 介绍Transformer

\- 怎么避免局部最优？

\- 介绍YOLOv1-YOLOv5

\- Python 字典底层怎么实现的

\- 分类问题，类别数特别大怎么办？

\- 介绍相机标定的原理和步骤

\- 模型太大，上线应该怎么办？

\- K-means过程、k值如何选择

\- 随机森林相对于决策树的优缺点

\- 编程：实现NMS

\- 条件随机场的能量函数

\- 介绍下AUC和F1-score

\- 为什么一般都是用奇数卷积？

\- 简述MobileNet V1,V2,V3的区别

\- 介绍NMS及其变体

\- 介绍mAP，具体怎么计算？

\- domain shift问题怎么解决？

\- 出现漏检、误检，怎么解决？



\- CRF和HMM的区别

\- 积分图均值滤波如何实现

\- 样本不平衡处理方法有哪些？

\- 怎么从Anchor变成具体坐标的？

\- YOLO v4相对于YOLO v3的改进？



\- 介绍DBSCAN原理

\- CNN反向传播怎么求导

\- 多标签分类要用什么损失函数？

\- Bagging和Boosting的区别与联系

\- 介绍Adam和AdaDelta

\- DQN的伪代码和流程图

\- 平衡二叉树判定，要求最优解法

\- GBDT为什么用CART，能不能用其他的

\#面试经验# 腾讯/字节跳动/美团/百度/滴滴/旷视/bilibili

\- LSTM相比RNN改进在哪里？

\- lightgbm相对xgboost的改进

\- 介绍PCA，特征值分解，奇异值分解

\- Focal Loss原理

\- 为什么静态图比动态图速度快？

\- GRU 和 Transformer 各自的优势

\- 优化器，从SGD到Adam，为什么需要自适应学习率？



\- 介绍U-Net及其变体

\- 写一下smooth L1的公式

\- 1*1的卷积核有哪些作用？

\- 介绍CAM模型可视化及其变体

\- Adam怎么实现自适应学习率？

\- 数据增广方法有哪些？

\- 介绍Caffe中卷积的实现

\- pooling 层怎么反向传播？

\- 图像处理中平滑和锐化操作是什么？

\- Attention的起源，以及是用在哪里？

\- 介绍直方图均衡化

\- 手撕IoU和NMS代码

\- 介绍图像的高频、低频分量/信号

\- 生成模型和判别模型分别有哪些？

\- 如何解决多标签分类问题？介绍损失函数

\- 介绍高斯滤波的步骤

\- 单应性（homography）原理

\- BN 可以防止过拟合么？为什么

\- 如何做目标检测的多尺度训练/测试？

\- Squeeze-Excitation结构怎么实现的？

\- 加速网络收敛的办法

\- 哪几种滤波器是平滑的？

\- 介绍label assigning方面的工作

\- 介绍跨卡同步Batch Normalization

\- YOLOv4使用的数据增广有哪些方法？

\- 手撕logistic回归

\- 如何实现Dropout？

\- Self-attention原理

\- 如何判断聚类效果的好坏？

\- 介绍并评价目标检测新范式DETR？

\- 写BatchNorm的公式

\- 聚类评价指标有哪些？

\- 较新的激活函数有哪些？

\- YOLOv3中 route层的作用是什么？

\- 介绍IoU、GIoU、DIoU、CIoU损失函数

\- 什么是图像直方图？

\- 权重初始化方法有哪些？

\- BN层反向传播，怎么求导？

\- 介绍mAP、PR曲线、AUC的定义

\- 普通卷积、DW PW卷积计算量推导

\- 介绍常见的聚类方法

\- 自监督和无监督的区别

\- 检测任务是分类任务还是回归任务？

\- RPN 中正类和负类的筛选阈值是什么？

\- 要同时使用BN和dropout，该如何使用？

\- 如何实现多任务网络？

\- One-hot有什么作用？

\- 介绍Caffe框架的工厂模式

\- SVM推导以及如何解决多分类问题

\- anchor的意义，anchor-free的意义

\- Sobel算子介绍一下

\- BN层的定义以及功能（写公式）

\- 解决数据不平衡的损失函数有哪些？

\- Focal Loss解决什么问题？有多少超参数？如何调参？

\- 写一下Self-attention公式

\- 朴素贝叶斯的朴素是什么意思？

\- 介绍AUC值，PR曲线，ROC曲线

\- 卷积神经网络的卷积核为什么大多是方形？而且是奇数？

\- 写一下mAP公式

\- 画一下VGG-16网络

\- 手撕K-means伪代码

\- 最大池化和均值池化适用的场景？

\- ShuffleNetv2相比v1有什么改进？

\- 怎么解决长尾问题？

\- 神经网络如何跳出局部最优？

\- 画一下ShuffleNetv2的结构

\- Channel Attention怎么实现的？

\- 多分类如果有 10000 类别，怎么优化？



\- 手撕XGBoost算法

\- 介绍开运算、闭运算

\- 介绍双线性插值（画图）

\- Early stopping怎么做的？

\- SVM核函数的作用，如何选取核函数？

\- 介绍DBSCAN聚类算法

\- 介绍分割网络的损失函数

\- 介绍目标检测中的多尺度训练/测试

\- bbox目标大小尺寸差异巨大怎么解决？损失函数上怎么设计？

\- 介绍霍夫变换（画出来）

\- pooling 层怎么反向传播？

\- Attention起源是用在哪里？

\- 介绍Mask R-CNN的RoI Align

\- ResNet第二个版本做了哪些改进？

\- 不同卷积核大小意义

\- 神经网络如何可视化？

\- CART树怎么分裂节点？

\- YOLOv3中bbox坐标回归怎么做的？和Faster R-CNN有什么区别？



\- 不平衡样本怎么处理？

\- 介绍常见的激活函数（画出来）

\- 目标检测中正负样本如何选取？

\- K-means的初始中心怎么优化？

\- CenterNet的heatmap怎么生成？

\- 绍介‬‬SIFT的理原‬‬和特性

\- AUC法方算计‬‬与Python现实‬‬

\- 画一下ResNet和VGG网络（画简‬‬即可）

\- 3*3 换变何几‬‬矩阵中的参数是什么含义？

\- 你做标目‬‬检测一般用什么损失函数？么什为‬‬用这个？



\- 绍介‬‬SIFT的理原‬‬和特性

\- AUC法方算计‬‬与Python现实‬‬

\- 画一下ResNet和VGG网络（画简‬‬即可）

\- 3*3 换变何几‬‬矩阵中的参数是什么含义？

\- 你做标目‬‬检测一般用什么损失函数？么什为‬‬用这个？

\- 1*1卷积的作用

\- 介绍边缘检测算子

\- 用NumPy实现卷积

\- 手写高斯滤波以及优化

\- 思考题：褶皱的纸，如何变形至平坦？

\- 核函数有哪些？

\- 介绍PCA与SVD

\- 如何计算网络的参数量

\- 介绍感受野含义和计算公式

\- 卷积神经网络提取的特征是什么?

\- 介绍三种图像插值方法

\- CNN网络中的不变性理解

\- One stage与Two Stage的区别

\- ResNet 残差网络解决的是什么问题？为什么能解决？

\- 介绍HOG算法

\- 知道哪些损失函数？

\- 介绍你知道的优化器

\- 训练时出现loss NAN的可能因素

\- 介绍Attention的QKV，怎么计算的？

\- GDBT的基本原理

\- 推导神经网络链式法则

\- 手撕代码：二叉树最大宽度

\- 模型加速优化的方法有哪些？

\- 介绍Adam优化算法，有哪些参数？

\- 手推SVM

\- RF和GBDT的区别

\- 常见的色彩空间有哪些？

\- point wise 和pair wise的优缺点

\- 反转链表，要求空间复杂度O(1)

\- 介绍膨胀卷积

\- 手推卷积的过程

\- Mini-batch的作用

\- 介绍相机内参外参标定原理

\- 逻辑回归为什么不用MSE做损失函数



\- 学习率调节策略

\- SVM引入核函数本质？

\- 决策树怎么选择特征？

\- C++中什么是左值什么是右值

\- 图像质量评价指标PSNR、SSIM

\- 介绍BN/LN/IN/GN

\- 介绍反卷积及其应用

\- 如何解决数据少的问题？

\- 从两种数学角度解释L1和L2

\- 算法题：两字符串最长公共子序列



\- 介绍Transformer

\- 怎么避免局部最优？

\- 介绍YOLOv1-YOLOv5

\- Python 字典底层怎么实现的

\- 分类问题，类别数特别大怎么办？

\- 介绍相机标定的原理和步骤

\- 模型太大，上线应该怎么办？

\- K-means过程、k值如何选择

\- 随机森林相对于决策树的优缺点

\- 编程：实现NMS

\- 条件随机场的能量函数

\- 介绍下AUC和F1-score

\- 为什么一般都是用奇数卷积？

\- 简述MobileNet V1,V2,V3的区别

\- 介绍NMS及其变体

\- 介绍mAP，具体怎么计算？

\- domain shift问题怎么解决？

\- 出现漏检、误检，怎么解决？



\- CRF和HMM的区别

\- 积分图均值滤波如何实现

\- 样本不平衡处理方法有哪些？

\- 怎么从Anchor变成具体坐标的？

\- YOLO v4相对于YOLO v3的改进？



\- 介绍DBSCAN原理

\- CNN反向传播怎么求导

\- 多标签分类要用什么损失函数？

\- Bagging和Boosting的区别与联系

\- 介绍Adam和AdaDelta

\- DQN的伪代码和流程图

\- 平衡二叉树判定，要求最优解法

\- GBDT为什么用CART，能不能用其他的

\#面试经验# 腾讯/字节跳动/美团/百度/滴滴/旷视/bilibili

\- LSTM相比RNN改进在哪里？

\- lightgbm相对xgboost的改进

\- 介绍PCA，特征值分解，奇异值分解

\- Focal Loss原理

\- 为什么静态图比动态图速度快？

\- GRU 和 Transformer 各自的优势

\- 优化器，从SGD到Adam，为什么需要自适应学习率？



## 基础问题

1.简单介绍一下什么是卷积以及它的原理

2.池化在卷积神经网络里面有什么作用，在引进池化后解决了什么问题？

3.列举至少三种激活函数，分别阐述他们的优缺点

4.介绍一下感受野的概念，以及如何计算感受野的大小

5.什么是过拟合现象，在训练网络的过程中如何防止过拟合现象的发生？如果网络中出现过拟合，你该如何应对？

6.什么是欠拟合现象，网络欠拟合你该如何办？

当网络损失下降的很平稳，突然间损失开始出现剧烈波动时，请分析是什么原因导致的？

什么是梯度消失和梯度爆炸，分别阐述解决方法？

介绍一下L1、L2正则化的原理和作用？

归一化和标准化的方法有哪些？在神经网络中归一化和标准化的作用

简述一下数据增样、图像增强的常用方法

介绍一下经典的五种卷积神经网络（VGG、残差两个是重点，其他的看改进点就好

在很多卷积神经网络中都会用到1x1卷积核，他的作用是什么？

介绍一下卷积和全连接的区别，为什么在图像领域卷积的效果要比全连接好？

介绍一下目前主流的目标检测算法，并简单做个对比，你再项目中使用的是什么算法，为什么用它而不是其他的？

什么是anchor框，它在检测算法里面有什么作用？

你是按照什么比例来划分你的数据集、训练集、验证集和测试集在网络训练时分别起到什么作用？为什么有了测试集后还需要用到验证集？

你有没有关注最新的检测框架的论文进展，你对未来的检测框架发展有什么看法？

介绍一下yoloV3，SSd、Faster-RCNN的思想和实现过程

BN的原理和作用？

如果你的网路不收敛，请分析一下什么情况导致的？

你是依据什么取评估你的网络性能？准确度、召回率或者mAP等

**如果让你去改进YOLOV3（Faster-RCNN、SSD），你要从哪方面入手**

如果网络不初始化权重，即初始权重为0，那么网络训练出来会是什么效果？

网络的评估标准mAP是什么，他是如何计算的？

batch_size大小对模型训练的影响

yolo和SSD以及Faster-RCNN的区别，大概意思就是问单阶段和两阶段的区别，为什么单阶段快，为什么双阶段准确度更高

除了以上算法外，你还了解哪些检测算法

你认为目标检测未来要解决的问题有哪些，有哪几个发展方向

你项目中遇到了哪些问题，你是如何解决的？

